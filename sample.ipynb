{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicFlux Generation Notebook\n",
    "\n",
    "This notebook converts the provided Python script into an interactive Jupyter Notebook format, optimized for Google Colab. It includes installation commands for all necessary dependencies."
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install einops Pillow diffusers transformers scipy\n",
    "!pip install git+https://github.com/yourusername/your-repo.git"  // Replace with actual repo if utils, train, constants are in a repo
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os \n",
    "import torch \n",
    "import argparse\n",
    "import math \n",
    "from einops import rearrange, repeat\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL\n",
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "from utils import load_t5, load_clap, load_ae\n",
    "from train import RF \n",
    "from constants import build_model"
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(t5, clip, img, prompt):\n",
    "    bs, c, h, w = img.shape\n",
    "    if bs == 1 and not isinstance(prompt, str):\n",
    "        bs = len(prompt)\n",
    "    \n",
    "    img = rearrange(img, \"b c (h ph) (w pw) -> b (h w) (c ph pw)\", ph=2, pw=2)\n",
    "    if img.shape[0] == 1 and bs > 1:\n",
    "        img = repeat(img, \"1 ... -> bs ...\", bs=bs)\n",
    "    \n",
    "    img_ids = torch.zeros(h // 2, w // 2, 3)\n",
    "    img_ids[..., 1] = img_ids[..., 1] + torch.arange(h // 2)[:, None]\n",
    "    img_ids[..., 2] = img_ids[..., 2] + torch.arange(w // 2)[None, :]\n",
    "    img_ids = repeat(img_ids, \"h w c -> b (h w) c\", b=bs)\n",
    "    \n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    txt = t5(prompt)\n",
    "    if txt.shape[0] == 1 and bs > 1:\n",
    "        txt = repeat(txt, \"1 ... -> bs ...\", bs=bs)\n",
    "    txt_ids = torch.zeros(bs, txt.shape[1], 3)\n",
    "\n",
    "    vec = clip(prompt)\n",
    "    if vec.shape[0] == 1 and bs > 1:\n",
    "        vec = repeat(vec, \"1 ... -> bs ...\", bs=bs)\n",
    "    \n",
    "    print(img_ids.size(), txt.size(), vec.size())\n",
    "    return img, {\n",
    "        \"img_ids\": img_ids.to(img.device),\n",
    "        \"txt\": txt.to(img.device),\n",
    "        \"txt_ids\": txt_ids.to(img.device),\n",
    "        \"y\": vec.to(img.device),\n",
    "    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('Generate with MusicFlux')\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    latent_size = (256, 16) \n",
    "    \n",
    "    model = build_model(args.version).to(device) \n",
    "    local_path = args.ckpt_path\n",
    "    state_dict = torch.load(local_path, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(state_dict['ema'])\n",
    "    model.eval()  # Important!\n",
    "    diffusion = RF()\n",
    "    \n",
    "    # Setup VAE and Vocoder\n",
    "    t5 = load_t5(device, max_length=256)\n",
    "    clap = load_clap(device, max_length=256)\n",
    "    \n",
    "    vae = AutoencoderKL.from_pretrained(os.path.join(args.audioldm2_model_path, 'vae')).to(device)\n",
    "    vocoder = SpeechT5HifiGan.from_pretrained(os.path.join(args.audioldm2_model_path, 'vocoder')).to(device)\n",
    "    \n",
    "    # Load prompts\n",
    "    with open(args.prompt_file, 'r') as f: \n",
    "        conds_txt = f.readlines()\n",
    "    L = len(conds_txt) \n",
    "    unconds_txt = [\"low quality, gentle\"] * L \n",
    "    print(L, conds_txt, unconds_txt) \n",
    "    \n",
    "    # Initialize noise\n",
    "    init_noise = torch.randn(L, 8, latent_size[0], latent_size[1]).cuda() \n",
    "    \n",
    "    STEPSIZE = 50\n",
    "    img, conds = prepare(t5, clap, init_noise, conds_txt)\n",
    "    _, unconds = prepare(t5, clap, init_noise, unconds_txt) \n",
    "    with torch.autocast(device_type='cuda'): \n",
    "        images = diffusion.sample_with_xps(model, img, conds=conds, null_cond=unconds, sample_steps = STEPSIZE, cfg = 7.0)\n",
    "    \n",
    "    print(images[-1].size())\n",
    "    \n",
    "    # Rearrange images\n",
    "    images = rearrange(\n",
    "        images[-1], \n",
    "        \"b (h w) (c ph pw) -> b c (h ph) (w pw)\",\n",
    "        h=128,\n",
    "        w=8,\n",
    "        ph=2,\n",
    "        pw=2)\n",
    "    # Normalize latents\n",
    "    latents = 1 / vae.config.scaling_factor * images\n",
    "    mel_spectrogram = vae.decode(latents).sample \n",
    "    print(mel_spectrogram.size()) \n",
    "    \n",
    "    # Generate waveforms and save as WAV files\n",
    "    for i in range(L): \n",
    "        x_i = mel_spectrogram[i]\n",
    "        if x_i.dim() == 4:\n",
    "            x_i = x_i.squeeze(1)\n",
    "        waveform = vocoder(x_i)\n",
    "        waveform = waveform[0].cpu().float().detach().numpy()\n",
    "        print(waveform.shape)\n",
    "        # Save waveform\n",
    "        from scipy.io import wavfile \n",
    "        os.makedirs('wav', exist_ok=True)\n",
    "        wavfile.write(f'wav/sample_{i}.wav', 16000, waveform)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments (since argparse doesn't work well in notebooks)\n",
    "class Args:\n",
    "    version = \"small\"\n",
    "    prompt_file = 'config/example.txt'\n",
    "    ckpt_path = 'musicflow_s.pt'\n",
    "    audioldm2_model_path = '/maindata/data/shared/multimodal/public/dataset_music/audioldm2'\n",
    "    seed = 2024\n",
    "\n",
    "args = Args()\n",
    "main(args)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
