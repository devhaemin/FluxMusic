{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MusicFlux Generation in Google Colab\n\nThis notebook adapts your Python script for Google Colab, allowing you to generate audio using MusicFlux. Follow the steps below to set up the environment, upload necessary files, and run the generation process."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 1: Install Required Libraries\n\n# Install necessary packages\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n!pip install diffusers transformers einops Pillow scipy soundfile\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 2: Upload or Clone Custom Modules\n\n# Option 1: Upload custom modules directly\n# Use the file upload widget to upload `utils.py`, `train.py`, and `constants.py`\nfrom google.colab import files\nuploaded = files.upload()\n\n# Option 2: Clone from GitHub repository (uncomment and modify the URL if needed)\n# !git clone https://github.com/yourusername/your-repo.git\n# %cd your-repo\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 3: Mount Google Drive (Optional)\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# If using Google Drive, adjust paths accordingly\n# For example:\n# prompt_file_path = '/content/drive/MyDrive/path_to/example.txt'\n# ckpt_path = '/content/drive/MyDrive/path_to/musicflow_s.pt'\n# audioldm2_model_path = '/content/drive/MyDrive/path_to/audioldm2'\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 4: Upload Necessary Files (If Not Using Google Drive)\n\n# Upload prompt file\nuploaded_prompt = files.upload()\n\n# Upload model checkpoint\nuploaded_ckpt = files.upload()\n\n# Upload audioldm2 models (VAE and vocoder) as a zip file and unzip\n# !unzip path_to_audioldm2_models.zip -d audioldm2\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 5: Define Configuration Variables and Main Script\n\nimport os \nimport torch \nimport math \nfrom einops import rearrange, repeat\nfrom PIL import Image\nfrom diffusers import AutoencoderKL\nfrom transformers import SpeechT5HifiGan\n\nfrom utils import load_t5, load_clap, load_ae\nfrom train import RF \nfrom constants import build_model\n\nfrom scipy.io import wavfile\n\n# Configuration Variables\nversion = \"small\"\nprompt_file_path = 'example.txt'  # Ensure this file is uploaded or provide the correct path\nckpt_path = 'musicflow_s.pt'      # Ensure this file is uploaded or provide the correct path\naudioldm2_model_path = '/content/audioldm2'  # Update the path based on your setup\nseed = 2024\n\n# If using Google Drive, update paths accordingly\n# prompt_file_path = '/content/drive/MyDrive/path_to/example.txt'\n# ckpt_path = '/content/drive/MyDrive/path_to/musicflow_s.pt'\n# audioldm2_model_path = '/content/drive/MyDrive/path_to/audioldm2'\n\ndef prepare(t5, clip, img, prompt):\n    bs, c, h, w = img.shape\n    if bs == 1 and not isinstance(prompt, str):\n        bs = len(prompt)\n\n    img = rearrange(img, \"b c (h ph) (w pw) -> b (h w) (c ph pw)\", ph=2, pw=2)\n    if img.shape[0] == 1 and bs > 1:\n        img = repeat(img, \"1 ... -> bs ...\", bs=bs)\n\n    img_ids = torch.zeros(h // 2, w // 2, 3)\n    img_ids[..., 1] = img_ids[..., 1] + torch.arange(h // 2)[:, None]\n    img_ids[..., 2] = img_ids[..., 2] + torch.arange(w // 2)[None, :]\n    img_ids = repeat(img_ids, \"h w c -> b (h w) c\", b=bs)\n\n    if isinstance(prompt, str):\n        prompt = [prompt]\n    txt = t5(prompt)\n    if txt.shape[0] == 1 and bs > 1:\n        txt = repeat(txt, \"1 ... -> bs ...\", bs=bs)\n    txt_ids = torch.zeros(bs, txt.shape[1], 3)\n\n    vec = clip(prompt)\n    if vec.shape[0] == 1 and bs > 1:\n        vec = repeat(vec, \"1 ... -> bs ...\", bs=bs)\n\n    print(img_ids.size(), txt.size(), vec.size())\n    return img, {\n        \"img_ids\": img_ids.to(img.device),\n        \"txt\": txt.to(img.device),\n        \"txt_ids\": txt_ids.to(img.device),\n        \"y\": vec.to(img.device),\n    }\n\ndef main():\n    print('Generate with MusicFlux')\n    torch.manual_seed(seed)\n    torch.set_grad_enabled(False)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    latent_size = (256, 16) \n\n    model = build_model(version).to(device) \n    local_path = ckpt_path\n    state_dict = torch.load(local_path, map_location=lambda storage, loc: storage)\n    model.load_state_dict(state_dict['ema'])\n    model.eval()  # important! \n    diffusion = RF()\n\n    # Setup VAE\n    t5 = load_t5(device, max_length=256)\n    clap = load_clap(device, max_length=256)\n\n    vae = AutoencoderKL.from_pretrained(os.path.join(audioldm2_model_path, 'vae')).to(device)\n    vocoder = SpeechT5HifiGan.from_pretrained(os.path.join(audioldm2_model_path, 'vocoder')).to(device)\n\n    # Read prompts\n    with open(prompt_file_path, 'r') as f: \n        conds_txt = f.readlines()\n    L = len(conds_txt) \n    unconds_txt = [\"low quality, gentle\"] * L \n    print(L, conds_txt, unconds_txt) \n\n    init_noise = torch.randn(L, 8, latent_size[0], latent_size[1]).to(device) \n\n    STEPSIZE = 50\n    img, conds = prepare(t5, clap, init_noise, conds_txt)\n    _, unconds = prepare(t5, clap, init_noise, unconds_txt) \n    with torch.autocast(device_type='cuda'): \n        images = diffusion.sample_with_xps(model, img, conds=conds, null_cond=unconds, sample_steps = STEPSIZE, cfg = 7.0)\n    \n    print(images[-1].size())\n    \n    images = rearrange(\n        images[-1], \n        \"b (h w) (c ph pw) -> b c (h ph) (w pw)\",\n        h=128,\n        w=8,\n        ph=2,\n        pw=2,)\n    # print(images.size())\n    latents = 1 / vae.config.scaling_factor * images\n    mel_spectrogram = vae.decode(latents).sample \n    print(mel_spectrogram.size()) \n    \n    os.makedirs('wav', exist_ok=True)  # Create directory to save WAV files\n\n    for i in range(L): \n        x_i = mel_spectrogram[i]\n        if x_i.dim() == 4:\n            x_i = x_i.squeeze(1)\n        waveform = vocoder(x_i)\n        waveform = waveform[0].cpu().float().detach().numpy()\n        print(waveform.shape)\n        wavfile.write(f'wav/sample_{i}.wav', 16000, waveform) \n\n    print(\"WAV files have been saved in the 'wav' directory.\")\n\n# Run the main function\nmain()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## **Step 6: Download Generated WAV Files**\n\nAfter the notebook finishes running, the generated WAV files will be saved in the `wav` directory. You can download them using the following code:"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Download generated WAV files\nimport shutil\nimport zipfile\n\ndef zip_dir(dir_path, zip_path):\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for root, dirs, files in os.walk(dir_path):\n            for file in files:\n                zipf.write(os.path.join(root, file),\n                           os.path.relpath(os.path.join(root, file), \n                           os.path.join(dir_path, '..')))\n\# Zip the 'wav' directory\nzip_dir('wav', 'wav_files.zip')\n\n# Download the zip file\nfiles.download('wav_files.zip')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## **Additional Notes**\n\n- **Using Google Drive:** If you mounted Google Drive, ensure that your file paths (`prompt_file_path`, `ckpt_path`, `audioldm2_model_path`) point to the correct locations in your Drive.\n- **Session Persistence:** Remember that Colab sessions are temporary. If you need persistent storage, utilize Google Drive to store your models and generated files.\n- **Handling Large Files:** For very large models or datasets, ensure you have sufficient Drive storage and adjust paths accordingly.\n- **Debugging:** Monitor the printed tensor sizes and shapes for debugging purposes. Ensure that all necessary files are correctly uploaded and paths are properly set."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
